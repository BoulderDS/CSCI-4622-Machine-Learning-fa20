{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Stochastic Gradient Descent\n",
    "***\n",
    "\n",
    "In this notebook we'll implement a rudimentary Stochastic Gradient Descent algorithm to learn the weights in simple linear regression.  Then we'll see if we can make it more efficient.  Finally, we'll investigate some graphical strategies for diagnosing convergence and tuning parameters. \n",
    "\n",
    "**Note**: There are some helper functions at the bottom of this notebook.  Scroll down and evaluate those before proceeding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Setting Up Simulated Data and a Sanity Check \n",
    "***\n",
    "\n",
    "We'll work with simulated data for this exercise where our generative model is given by \n",
    "\n",
    "$$\n",
    "Y = 1 + 2X + \\epsilon \\textrm{ where} \\epsilon \\sim N(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "**Part A**: The following function will generate data from the model. We'll grab a training set of size $n=100$ and a validation set of size $n = 50$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataGenerator(n, sigsq=1.0, random_state=1236):\n",
    "    np.random.seed(random_state)\n",
    "    x_train = np.linspace(-1, 1, n)\n",
    "    x_valid = np.linspace(-1, 1, int(n/4))\n",
    "    y_train = 1 + 2 * x_train + np.random.randn(n)\n",
    "    y_valid = 1 + 2 * x_valid + np.random.randn(int(n/4))\n",
    "    return x_train, x_valid, y_train, y_valid \n",
    "\n",
    "x_train, x_valid, y_train, y_valid = dataGenerator(100)\n",
    "    \n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "ax.scatter(x_train, y_train, color=\"steelblue\", s=100, label=\"train\")\n",
    "ax.scatter(x_valid, y_valid, color=\"#a76c6e\", s=100, label=\"valid\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_xlabel(\"X\", fontsize=16)\n",
    "ax.set_ylabel(\"Y\", fontsize=16)\n",
    "ax.legend(loc=\"upper left\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Since we're going to be implementing things ourselves, we're going to want to prepend the data matrices with a column of ones so we can fit a bias term.  We can do this using numpy's [column_stack](https://docs.scipy.org/doc/numpy/reference/generated/numpy.column_stack.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.column_stack((np.ones_like(x_train), x_train))\n",
    "X_valid = np.column_stack((np.ones_like(x_valid), x_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Finally, let's fit a linear regression model with sklearn's LinearRegression class and print the coefficients so we know what we're shooting for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "reg = LinearRegression(fit_intercept=False)\n",
    "reg.fit(X_train, y_train)\n",
    "print(\"sklearn says the coefficients are \", reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: The last thing we'll do is visualize the surface of the RSS, of which we're attempting to find the minimum. Does it looks like the parameters reported by sklearn lie at the bottom of the RSS surface?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotsurface(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Implementing and Improving SGD \n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Now it's time to implement Stochastic Gradient Descent.  Most of the code in the function sgd has been written for you.  Your job is to fill in the values of the partial derivatives in the appropriate places.  Recall that the update scheme is given by \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\beta_0 &\\leftarrow& \\beta_0 - \\eta \\cdot 2 \\cdot \\left[(\\beta_0 + \\beta_1x_i) -y_i \\right] \\\\\n",
    "\\beta_1 &\\leftarrow& \\beta_1 - \\eta \\cdot 2 \\cdot \\left[(\\beta_0 + \\beta_1x_i) -y_i \\right] x_i\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Note that the function parameter beta is a numpy array containing the initial guess for the solve. The numpy array bhist stores the approximation of the betas after each iteration for plotting and diagnostic purposes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X, y, beta, eta=0.1, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Peform Stochastic Gradient Descent \n",
    "    \n",
    "    :param X: matrix of training features \n",
    "    :param y: vector of training responses \n",
    "    :param beta: initial guess for the parameters\n",
    "    :param eta: the learning rate \n",
    "    :param num_epochs: the number of epochs to run \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize history for plotting \n",
    "    bhist = np.zeros((num_epochs+1, len(beta)))\n",
    "    bhist[0,0], bhist[0,1] = beta[0], beta[1]\n",
    "    \n",
    "    # perform epochs \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        \n",
    "        # shuffle indices \n",
    "        shuffled_inds = list(range(X.shape[0]))\n",
    "        np.random.shuffle(shuffled_inds)\n",
    "        \n",
    "        # loop over training examples \n",
    "        for index in shuffled_inds:\n",
    "            beta[0] = # TODO\n",
    "            beta[1] = # TODO \n",
    "\n",
    "        # save history \n",
    "        bhist[epoch, :] = beta\n",
    "        \n",
    "    # return bhist. Last row \n",
    "    # are the learned parameters. \n",
    "    return bhist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start at (-2,1)\n",
    "beta_start = np.array([-2.0, -1.0])\n",
    "\n",
    "# Training \n",
    "%time bhist = sgd(X_train, y_train, beta=beta_start, eta=0.0025, num_epochs=1000)\n",
    "\n",
    "# Print and Plot \n",
    "print(\"beta_0 = {:.5f}, beta_1 = {:.5f}\".format(bhist[-1][0], bhist[-1][1]))\n",
    "plotsurface(X_train, y_train, bhist=bhist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Thinking about the case where we have more than two features, can you think of a way to vectorize the stochastic gradient update of the parameters? When you see it, go back to the sgd function and improve it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X, y, beta, eta=0.1, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Peform Stochastic Gradient Descent \n",
    "    \n",
    "    :param X: matrix of training features \n",
    "    :param y: vector of training responses \n",
    "    :param beta: initial guess for the parameters\n",
    "    :param eta: the learning rate \n",
    "    :param num_epochs: the number of epochs to run \n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize history for plotting \n",
    "    bhist = np.zeros((num_epochs+1, len(beta)))\n",
    "    bhist[0,0], bhist[0,1] = beta[0], beta[1]\n",
    "    \n",
    "    # perform epochs \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        \n",
    "        # shuffle indices \n",
    "        shuffled_inds = list(range(X.shape[0]))\n",
    "        np.random.shuffle(shuffled_inds)\n",
    "        \n",
    "        # loop over training examples \n",
    "        for index in shuffled_inds:\n",
    "            # TODO\n",
    "\n",
    "        # save history \n",
    "        bhist[epoch, :] = beta\n",
    "        \n",
    "    # return bhist. Last row \n",
    "    # are the learned parameters. \n",
    "    return bhist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Now that you have created this beautiful solver, go back and break it by playing with the learning rate. Does the learning rate have the effect on convergence that you expect when visualized in the surface plot? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Graphical Diagnosis of Convergence \n",
    "***\n",
    "\n",
    "A common way to monitor the convergence of SGD and to tune hyperparameters (like learning rate and regularization strength) is to make a plot of how the loss function evolves during the training process. That is, we plot the value of the loss function periodically and see if it looks like it's reached a minimum, or see if it's jumping around a lot.  Normally we'd record the value of the loss function as we train, but we'll use the beta histories returned by our solver.  Finally, using the MSE instead of the RSS is a popular choice, so we'll do that.  \n",
    "\n",
    "**Part A**: Modify the function below to take in a beta history and a data set and return a vector of MSE values for each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_hist(X, y, bhist):\n",
    "    mse = np.zeros(bhist.shape[0])\n",
    "    for epoch in range(bhist.shape[0]):\n",
    "        mse[epoch] = # TODO\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Next we'll take the MSE history that we just computed and plot it vs epoch number. Based on your plot, would you say that your MSE has converged? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = MSE_hist(X_train, y_train, bhist)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "ax.plot(range(1,len(mse_train)+1), mse_train, color=\"steelblue\", label=\"train\")\n",
    "ax.set_xlabel(\"epoch number\", fontsize=16)\n",
    "ax.set_ylabel(\"mse\", fontsize=16);\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(loc=\"upper right\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Go back up and change the value of the learning rate to bigger and smaller values (you might also have to adjust the max epochs).  Do the different learning rates have the effect on the MSE plots that you would expect? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: OK, now for the real question.  Is the MSE on the training data the best thing to look at when deciding if our training algorithm has converged? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycolors = {\"blue\": \"steelblue\", \"red\":\"#a76c6e\",  \"green\":\"#6a9373\", \"smoke\": \"#f2f2f2\"}\n",
    "\n",
    "def eval_RSS(X, y, b0, b1):\n",
    "    rss = 0 \n",
    "    for ii in range(len(df)):\n",
    "        xi = df.loc[ii, \"x\"]\n",
    "        yi = df.loc[ii, \"y\"]\n",
    "        rss += (yi - (b0 + b1 * xi)) ** 2\n",
    "    return rss\n",
    "\n",
    "def plotsurface(X, y, bhist=None):\n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 300), np.linspace(-1, 5, 300))\n",
    "    Z = np.zeros((xx.shape[0], yy.shape[0]))\n",
    "    for ii in range(X.shape[0]):\n",
    "        Z += (y[ii] - xx - yy * X[ii,1]) ** 2\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,10))\n",
    "    levels = [125, 200] + list(range(400,2000,400))\n",
    "    CS = ax.contour(xx, yy, Z, levels=levels)\n",
    "    ax.clabel(CS, CS.levels, inline=True, fontsize=10)\n",
    "    ax.set_xlim([-3,3])\n",
    "    ax.set_ylim([-1,5])\n",
    "    ax.set_xlabel(r\"$\\beta_0$\", fontsize=20)\n",
    "    ax.set_ylabel(r\"$\\beta_1$\", fontsize=20)\n",
    "    if bhist is not None:\n",
    "        for ii in range(bhist.shape[0]-1):\n",
    "            x0 = bhist[ii][0]\n",
    "            y0 = bhist[ii][1]\n",
    "            x1 = bhist[ii+1][0]\n",
    "            y1 = bhist[ii+1][1]\n",
    "            ax.plot([x0, x1], [y0, y1], color=\"black\", marker=\"o\", lw=1.5, markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
