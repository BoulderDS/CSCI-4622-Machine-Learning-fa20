{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Support Vector Machines\n",
    "***\n",
    "\n",
    "In this notebook we'll explore the details of the Soft-Margin SVM and look at how the choice of tuning parameters  affects the learned models.  We'll also look at kernel SVMs for non-linearly separable and methods for choosing and visualizing good hyperparameters.   \n",
    "\n",
    "**Note**: There are some helper functions at the bottom of this notebook.  Scroll down and execute those cells before continuing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T05:43:34.800503Z",
     "start_time": "2018-04-13T05:43:34.395148Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Soft-Margin SVM Details\n",
    "***\n",
    "\n",
    "Suppose you have the following labeled data set (assume here that red corresponds to $y=1$ and blue corresponds to $y = -1$) and suppose the SVM decision boundary is defined by the weights ${\\bf w} = [-1/4, ~ 1/4]^T$ and $b = -1/4$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T05:43:46.504143Z",
     "start_time": "2018-04-13T05:43:46.266153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data and Labels \n",
    "X = np.array([[1,8],[7,2],[6,-1],[-5,0], [-5,1], [-5,2],[6,3],[6,1],[5,2]])\n",
    "y = np.array([1,-1,-1,1,-1,1,1,-1,-1])\n",
    "\n",
    "# Support vector parameters \n",
    "w, b = np.array([-1/4, 1/4]), -1/4\n",
    "# w, b = np.array([-1/2, 1/2]), -1/2\n",
    "\n",
    "# Plot the data and support vector boundaries \n",
    "linear_plot(X, y, w=w, b=b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: What is the margin of this particular SVM? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Which training examples are the support vectors? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Which training examples have nonzero slack? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Compute the slack $\\xi_i$ associated with the misclassified points. Do these values jive with the plot of the data and the support vector boundaries? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Suppose we obtained the parameters ${\\bf w}$ and $b$ by solving the Dual optimization problem.  Which of the following are possible realizations of the Lagrange multipliers? Here $C$ is the hyperparameter associated with the slack penalty term in the primal objective function. \n",
    "\n",
    "$\n",
    "\\texttt{i}) ~~~~\\quad \\alpha^T = \\left[ \\begin{array}{r} 1.0 & 1.5 & 0.5 & 0.75 & 0.25 & 2.0 & 1.5 & 0.25 & 0.5 \\end{array} \\right] \n",
    "$\n",
    "\n",
    "$\n",
    "\\texttt{ii})~~ \\quad \\alpha^T = \\left[ \\begin{array}{r} C/2 & C & 0 & 0 & C & 0 & 0 & C/2& 0 \\end{array} \\right]\n",
    "$\n",
    "\n",
    "$\n",
    "\\texttt{iii}) \\quad \\alpha^T = \\left[ \\begin{array}{r} C/3 & C & 0 & 0 & C & 0 & 0 & C/2 & 0 \\end{array} \\right]\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: The Margin vs Slack \n",
    "***\n",
    "\n",
    "In this problem we'll figure out how to fit linear SVM models to data using sklearn.  Consider the data shown below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T05:44:00.155106Z",
     "start_time": "2018-04-13T05:43:59.912927Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = part2data()\n",
    "linear_plot(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Let's fit a linear Soft-Margin SVM to the data above. For SVMs with a linear kernel we'll use the [`LinearSVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) method from sklearn's `svm` module.  Go now and look at the documentation. \n",
    "\n",
    "Recall that the primal objective function for the linear kernel SVM is as follows \n",
    "\n",
    "\n",
    "$$\n",
    "\\min_{{\\bf w}, b, {\\bf \\xi}} \\frac{1}{2}\\|{\\bf w}\\|^2 + C \\sum_{i=1}^m \\xi_i^p\n",
    "$$\n",
    "\n",
    "The two optional parameters in `LinearSVC` that we'll be most concerned with are `C`, the hyperparameter weighting the slackness contribution to the primal objective function, and `loss`, which determines the exponent on the slack variables in the sum. \n",
    "\n",
    "Write some code below to train a linear SVM with $C=1$ and $p=1$, get the computed weight vector and bias, and the plot the resulting model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T05:44:05.182461Z",
     "start_time": "2018-04-13T05:44:04.884759Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# TODO: Train the model and get the parameters, pay attention to the loss parameter\n",
    "\n",
    "\n",
    "linear_plot(X, y, w=w, b=b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Experiment with different values of `C`.  How does the choice of `C` affect the nature of the decision boundary and the associated margin? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: In general, how does the choice of `C` affect the bias and variance of the model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Nonlinear SVM, Parameter Tuning, Accuracy, and Cross-Validation \n",
    "***\n",
    "\n",
    "Any support vector machine classifier will have at least one parameter that needs to be tuned based on the training data.  The guaranteed parameter is the $C$ associated with the slack variables in the primal objective function, i.e. \n",
    "\n",
    "$$\n",
    "\\min_{{\\bf w}, b, {\\bf \\xi}} \\frac{1}{2}\\|{\\bf w}\\|^2 + C \\sum_{i=1}^m \\xi_i\n",
    "$$\n",
    "\n",
    "If you use a kernel fancier than the linear kernel then you will likely have other parameters as well. For instance in the polynomial kernel $K({\\bf x}, {\\bf z}) = ({\\bf x}^T{\\bf z} + c)^d$ you have to select the shift $c$ and the polynomial degree $d$.  Similarly the rbf kernel\n",
    "\n",
    "$$\n",
    "K({\\bf x}, {\\bf z}) = \\exp\\left[-\\gamma\\|{\\bf x} - {\\bf z}\\|^2\\right]\n",
    "$$\n",
    "\n",
    "has one tuning parameter, namely $\\gamma$, which controls how fast the similarity measure drops off with distance between ${\\bf x}$ and ${\\bf z}$. \n",
    "\n",
    "For our examples we'll consider the rbf kernel, which gives us two parameters to tune, namely $C$ and $\\gamma$. \n",
    "\n",
    "Consider the following two dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T05:44:16.424519Z",
     "start_time": "2018-04-13T05:44:16.216386Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = part3data(N=300, seed=1235)\n",
    "nonlinear_plot(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: We can use the method [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) from sklearn's `svm` module to fit an SVM with a nonlinear kernel to the data.  Go now and look at the documentation. Note that we pass the `kernel=\"rbr\"` parameter to use the RBF kernel.  The other two parameters we'll be concerned with are `C` and the RBF parameter `gamma`.   \n",
    "\n",
    "Write some code to fit an SVM with RBF kernel to the data and plot the results.  Use the parameter values `C=1` and `gamma=1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T05:44:21.586000Z",
     "start_time": "2018-04-13T05:44:21.308315Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "nlsvm = # TODO\n",
    "\n",
    "nonlinear_plot(X, y, nlsvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: In this part we'll use cross-validation to estimate the validation accuracy achieved by our model.  Experiment with the values of the hyperparameters to see if you can get a good validation accuracy. How do the choice of `C` and `gamma` affect the resulting decision boundary? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T05:44:24.228772Z",
     "start_time": "2018-04-13T05:44:23.934095Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# TODO: use cross_val_score to estmiate the cross validation accuracy\n",
    "print(\"cross-val mean-accuracy: {:.3f}\".format(np.mean(scores)))\n",
    "\n",
    "nonlinear_plot(X, y, nlsvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: How does the choice of **kernel** function affect the bias/variance of the model?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Automating the Parameter Search \n",
    "***\n",
    "\n",
    "On the previous problem we were able to choose some OK parameters just by hand-tuning.  But in real life (where time is money) it would be better to do something a little more automated.  One common thing to do is a **grid-search** over a predefined range of the parameters.  In this case you will loop over all possible combinations of parameters, estimate the accuracy of your model using K-Folds cross-validation, and then choose the parameter combination that produces the highest validation accuracy. \n",
    "\n",
    "**Part A**: Below is an experiment where we search over a logarithmic range between $2^{-5}$ and $2^{5}$ for $C$ and a range between $2^{-5}$ and $2^{5}$ for $\\gamma$.  For the accuracy measure we use K-Folds CV with $K=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T05:45:55.092532Z",
     "start_time": "2018-04-13T05:44:28.657367Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "c_range = np.logspace(-5, 5, 11, base=2)\n",
    "g_range = np.logspace(-5, 5, 11, base=2)\n",
    "# TODO: use GridSearchCV to find the best parameter using cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: The following function will plot a heat-map of the cross-validation accuracies for each combination of parameters.  Which combination looks the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T05:47:45.970394Z",
     "start_time": "2018-04-13T05:47:45.624952Z"
    }
   },
   "outputs": [],
   "source": [
    "plotSearchGrid(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: The GridSearchCV object scores, among other things, the best combination of parameters as well as the cross-validation accuracy achieved with those parameters.  Print those quantities for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: The GridSearchCV object also stores the classifier trained with the best hyperparameters.  Pass this best estimator into the `nonlinear_plot` function to view the best decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T05:43:41.135235Z",
     "start_time": "2018-04-13T05:43:39.601395Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def linear_plot(X, y, w=None, b=None):\n",
    "    \n",
    "    mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "    colors = [mycolors[\"red\"] if yi==1 else mycolors[\"blue\"] for yi in y]\n",
    "    \n",
    "    # Plot data \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,8))\n",
    "    ax.scatter(X[:,0], X[:,1], color=colors, s=150, alpha=0.95, zorder=2)\n",
    "    \n",
    "    # Plot boundaries \n",
    "    lower_left = np.min([np.min(X[:,0]), np.min(X[:,1])])\n",
    "    upper_right = np.max([np.max(X[:,0]), np.max(X[:,1])])\n",
    "    gap = .1*(upper_right-lower_left)\n",
    "    xplot = np.linspace(lower_left-gap, upper_right+gap, 20)\n",
    "    if w is not None and b is not None: \n",
    "        ax.plot(xplot, (-b - w[0]*xplot)/w[1], color=\"gray\", lw=2, zorder=1)\n",
    "        ax.plot(xplot, ( 1 -b - w[0]*xplot)/w[1], color=\"gray\", lw=2, ls=\"--\", zorder=1)\n",
    "        ax.plot(xplot, (-1 -b - w[0]*xplot)/w[1], color=\"gray\", lw=2, ls=\"--\", zorder=1)\n",
    "        \n",
    "    \n",
    "    ax.set_xlim([lower_left-gap, upper_right+gap])\n",
    "    ax.set_ylim([lower_left-gap, upper_right+gap])\n",
    "    \n",
    "    ax.grid(alpha=0.25)\n",
    "    \n",
    "def part2data():\n",
    "    \n",
    "    np.random.seed(1239)\n",
    "    \n",
    "    X = np.zeros((22,2))\n",
    "    X[0:10,0]  = 1.5*np.random.rand(10) \n",
    "    X[0:10,1]  = 1.5*np.random.rand(10)\n",
    "    X[10:20,0] = 1.5*np.random.rand(10) +  1.75\n",
    "    X[10:20,1] = 1.5*np.random.rand(10) +  1\n",
    "    X[20,0] = 1.5\n",
    "    X[20,1] = 2.25\n",
    "    X[21,0] = 1.6\n",
    "    X[21,1] = 0.25\n",
    "    \n",
    "    y = np.ones(22)\n",
    "    y[10:20] = -1 \n",
    "    y[20] = 1\n",
    "    y[21] = -1\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def part3data(N=100, seed=1235):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X = np.random.uniform(-1,1,(N,2))\n",
    "    y = np.array([1 if y-x > 0 else -1 for (x,y) in zip(X[:,0]**2 * np.sin(2*np.pi*X[:,0]), X[:,1])])\n",
    "    X = X + np.random.normal(0,.1,(N,2))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def nonlinear_plot(X, y, clf=None): \n",
    "    \n",
    "    mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,10))\n",
    "    \n",
    "    colors = [mycolors[\"red\"] if yi==1 else mycolors[\"blue\"] for yi in y]\n",
    "    ax.scatter(X[:,0],X[:,1], marker='o', color=colors, s=100, alpha=0.5)\n",
    "    \n",
    "    ax.arrow(-1.25,0,2.5,0, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    ax.arrow(0,-1.25,0,2.5, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    z = np.linspace(0.25,3.5,10)\n",
    "    \n",
    "    ax.set_xlim([-1.50,1.50])\n",
    "    ax.set_ylim([-1.50,1.50])\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    plt.xticks([], fontsize=16)\n",
    "    plt.yticks([], fontsize=16)\n",
    "    \n",
    "\n",
    "    if clf: \n",
    "        \n",
    "        clf.fit(X,y)\n",
    "\n",
    "        x_min = X[:, 0].min()+.00\n",
    "        x_max = X[:, 0].max()-.00\n",
    "        y_min = X[:, 1].min()+.00\n",
    "        y_max = X[:, 1].max()-.00\n",
    "\n",
    "        colors = [mycolors[\"red\"] if yi==1 else mycolors[\"blue\"] for yi in y]\n",
    "\n",
    "        XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "        Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(XX.shape)\n",
    "        plt.contour(XX, YY, Z, colors=[mycolors[\"blue\"], \"gray\", mycolors[\"red\"]], linestyles=['--', '-', '--'],\n",
    "                    levels=[-1.0, 0, 1.0], linewidths=[2,2,2], alpha=0.9)\n",
    "    \n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "    \n",
    "def plotSearchGrid(grid):\n",
    "    \n",
    "    scores = [x for x in grid.cv_results_[\"mean_test_score\"]]\n",
    "    scores = np.array(scores).reshape(len(grid.param_grid[\"C\"]), len(grid.param_grid[\"gamma\"]))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "    plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n",
    "               norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\n",
    "    plt.xlabel('gamma')\n",
    "    plt.ylabel('C')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(grid.param_grid[\"gamma\"])), grid.param_grid[\"gamma\"], rotation=45)\n",
    "    plt.yticks(np.arange(len(grid.param_grid[\"C\"])), grid.param_grid[\"C\"])\n",
    "    plt.title('Validation accuracy')\n",
    "    plt.show()\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
