{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Validation and Evaluation Metrics Solutions\n",
    "***\n",
    "\n",
    "In this notebook we'll investigate Scikit-Learn's functionality for performing cross-validation, plotting ROC curves, and plotting learning curves. \n",
    "\n",
    "**Note**: There are some helper functions at the bottom of this notebook.  Scroll down and execute those cells before continuing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T20:01:07.337144Z",
     "start_time": "2018-03-16T20:01:06.227417Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data Examination and Visualization\n",
    "***\n",
    "\n",
    "The data we will explore in this notebook is the so-called Spambase data, which contains features extracted from SPAM and HAM emails.  The following cell will load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T20:01:12.937044Z",
     "start_time": "2018-03-16T20:01:12.879023Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/spamdata.csv\", sep=\" \")\n",
    "X, y = data.values[:,:-1], data.values[:,-1]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: The features in in this dataset are a combination of frequency counts for select words as well as other numerical features derived from the original email text.  Some of the word-count features included are things like \n",
    "\n",
    "- `word_freq_order`: percentage of words in the email that are the word `order` \n",
    "- `word_freq_free`: percentage of words in the email that are the word `free` \n",
    "\n",
    "A few other relevant features are things like \n",
    "\n",
    "- `capital_run_length_average`: the average length of a run of capital letters \n",
    "- `capital_run_length_longest`: the longest length of a run of capital letters \n",
    "- `char_freq_!`: the number of exclamation points that appear in the email \n",
    "\n",
    "Descriptions of the rest of the features can be found [here](http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names). \n",
    "\n",
    "**Part B**: \n",
    "\n",
    "Let's visualize some of the features by plotting histograms of the features colored by whether the email is SPAM or HAM. It seems like the number of contiguous capital letters should be a good indicator of SPAM... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T04:48:05.417404Z",
     "start_time": "2018-03-16T04:48:05.009534Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_hist(data, \"capital_run_length_longest\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And probably also the number of exclamation points ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T04:48:07.097336Z",
     "start_time": "2018-03-16T04:48:06.716749Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_hist(data, \"char_freq_!\", 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Estimating Model Performance with Cross-Validation \n",
    "***\n",
    "\n",
    "In this section we'll use sklearn's built in cross-validation routine to estimate the accuracy of logistic regression for our data set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: First, let's set a baseline by performing a train-validation split on the data and then fitting a logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T20:07:22.595095Z",
     "start_time": "2018-03-16T20:07:22.039228Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1001)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=0.001)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll evaluate the error on the validation set and see how well we did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T04:48:12.878525Z",
     "start_time": "2018-03-16T04:48:12.874203Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_test_score = # TODO\n",
    "print(\"test accuracy: {:.3f}\".format(mean_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Now we will use cross validation to choose the best parameter. Try it now with $k=5$ folds. \n",
    "\n",
    "Now let us first run cross validation with default parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T17:34:12.743469Z",
     "start_time": "2018-03-16T17:34:12.485174Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate \n",
    "\n",
    "scores =  # TODO\n",
    "mean_train_score = # TODO \n",
    "mean_test_score =  # TODO \n",
    "print(\"mean train score: {:.3f} test score: {:.3f}\".format(mean_train_score, mean_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Now let us try different parameters and see which parameter gives the best cross validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(5, -5, 11, base=2)\n",
    "best_score, best_lambda = 0, None\n",
    "mean_train_scores, mean_test_scores = [], []\n",
    "\n",
    "# TODO\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,8))\n",
    "ax.loglog(lambdas, mean_test_scores, color=mycolors[\"blue\"])\n",
    "ax.loglog(lambdas, mean_train_scores, color=mycolors[\"red\"])\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_xlabel(\"lambda\", fontsize=32)\n",
    "ax.set_ylabel(\"accuracy\", fontsize=32)\n",
    "ax.legend([\"test\", \"train\"], fontsize=32)\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Use best lambda to train a classifier and run on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=best_lambda)\n",
    "logreg.fit(X_train, y_train)\n",
    "print(\"test accuracy: {:.3f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Evaluating Model Performance with the ROC curve \n",
    "***\n",
    "\n",
    "Note that SPAM classification is an application in which we might want to finely tune the true positive rate and false positive rate of our classifier. \n",
    "\n",
    "**Part A**: Think about SPAM classification.  What kinds of classification errors would be the most detrimental? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Let's plot a ROC curve for our Logistic Regression classifier. Sklearn's [roc_curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) routine computes the FPR and TPR for a range of possible thresholds in the data which you can then use to plot. Check out the documentation, and then fill in the code below to plot the curve for our Logistic Regression SPAM classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T17:32:06.059275Z",
     "start_time": "2018-03-16T17:32:05.810114Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "logreg = LogisticRegression(C=best_lambda) \n",
    "logreg.fit(X_train, y_train)\n",
    "y_test_scores = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# TODO\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,8))\n",
    "\n",
    "# TODO: plot TPR vs FPR\n",
    "\n",
    "ax.plot([0,1],[0,1], ls=\"--\", color=\"gray\")\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_xlabel(\"FPR\", fontsize=16)\n",
    "ax.set_ylabel(\"TPR\", fontsize=16);\n",
    "\n",
    "# TODO for Part C: find the TPR for FPR at 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Let's suppose you want to use your ROC curve to decide how your model would perform if you fixed highest the false positive rate that you're willing to accept for your classifier.  Modify your code above to overlay a red dot on the ROC curve at your desired FPR level.  What threshold does this correspond to? At this specific FPR, do you think our model is good enough to real-life SPAM classification?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>\n",
    "<br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-16T04:47:55.108552Z",
     "start_time": "2018-03-16T04:47:55.092411Z"
    }
   },
   "outputs": [],
   "source": [
    "mycolors = dict({\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\", \"orange\": \"orange\"})\n",
    "\n",
    "def feature_hist(df, feat, feat_max):\n",
    "    \"\"\"\n",
    "    Function to plot SPAM vs HAM histograms for a given feature \n",
    "    \n",
    "    :param df: the DataFrame \n",
    "    :param feat: the feature name \n",
    "    :param feat_max: the largest values of feature to plot \n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True, figsize=(10,8))\n",
    "    df.loc[df[feat] < feat_max].hist(column=feat, by=\"isSPAM\", ax=axes, bins=20, edgecolor=\"white\")\n",
    "    axes[0].set_title(\"HAM\", fontsize=16); axes[1].set_title(\"SPAM\", fontsize=16)\n",
    "    for ax in axes:\n",
    "        ax.grid(alpha=0.25)\n",
    "        ax.set_axisbelow(\"True\")\n",
    "        ax.set_xlabel(feat, fontsize=16)\n",
    "        ax.set_ylabel(\"frequency\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
